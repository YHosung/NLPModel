# -*- coding: utf-8 -*-
"""NLPModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1up2w6uV-IPP7uoj18aotjIaIcHLBohdu
"""

from google.colab import drive
drive.mount('/content/drive')

!wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tgz
!tar xvfz Python-3.7.0.tgz
!Python-3.7.0/configure
!make
!sudo make install

!python --version

!pip install mxnet
!pip install gluonnlp pandas tqdm
!pip install sentencepiece
!pip install transformers
!pip install torch

!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'

!pip install torch transformers sentencepiece
!pip install gluonnlp pandas tqdm
!pip install torch transformers gluonnlp tqdm pandas

#!pip install --upgrade gluonnlp mxnet

!pip uninstall mxnet -y
!pip uninstall gluonnlp -y
!pip install mxnet
!pip install gluonnlp

!pip install numpy==1.19.5 # Install an older version of NumPy where np.bool is still available

import numpy as np
import sys

if not hasattr(np, "bool"):
    # If np.bool is not present, create an alias to the built-in bool
    sys.modules['numpy'].bool = bool

# Rest of your code, including imports for gluonnlp, etc.
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
from tqdm import tqdm, tqdm_notebook

import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook

!pip install gluonnlp --upgrade --no-cache-dir

import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook

from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel

from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup
device = torch.device("cuda:0")
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')

#import torch
#from torch import nn
#import torch.nn.functional as F
#import torch.optim as optim
#from torch.utils.data import Dataset, DataLoader
#import numpy as np
#from tqdm import tqdm, tqdm_notebook

#from kobert_tokenizer import KoBERTTokenizer
#from transformers import BertModel

#from transformers import AdamW
#from transformers.optimization import get_cosine_schedule_with_warmup
#device = torch.device("cuda:0")
#tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
#bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
#vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')

#위에 오류 코드 바꾼 거
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm, tqdm_notebook

# Import gluonnlp
import gluonnlp as nlp  # This line is added to import gluonnlp

from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel

from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup
device = torch.device("cuda:0")
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')

!pip uninstall numpy --yes

import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm, tqdm_notebook

from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel

from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup

# Import gluonnlp
import gluonnlp as nlp  # This line is added to import gluonnlp

device = torch.device("cuda:0")
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')

#import torch
#from torch import nn
#import torch.nn.functional as F
#import torch.optim as optim
#from torch.utils.data import Dataset, DataLoader
#import numpy as np
#from tqdm import tqdm, tqdm_notebook

#from kobert_tokenizer import KoBERTTokenizer
#from transformers import BertModel

#from transformers import AdamW
#from transformers.optimization import get_cosine_schedule_with_warmup
#device = torch.device("cuda:0")
#tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
#bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
#vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')

import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook

from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel

from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup
device = torch.device("cuda:0")
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')

!pip install transformers==4.31.0
!pip install gluonnlp
!pip install mxnet
!pip install sentencepiece

#아래 코드 수정한 거
import pandas as pd
import os

# Check if the file exists
file_path = "/content/한국어_단발성_대화_데이터셋.xlsx"
if not os.path.exists(file_path):
    print(f"Error: File not found at {file_path}")
    # Here, you might add code to upload or mount the file
    # For example, in Google Colab:
    # from google.colab import files
    # uploaded = files.upload()
    # file_path = list(uploaded.keys())[0]
else:
    # If the file exists, proceed to read it
    chatbot_data = pd.read_excel(file_path)

    chatbot_data.sample(n=10)
    # 7개의 감정 class → 숫자
    chatbot_data.loc[(chatbot_data['Emotion'] == "공포"), 'Emotion'] = 0  #공포 => 0
    chatbot_data.loc[(chatbot_data['Emotion'] == "놀람"), 'Emotion'] = 1  #놀람 => 1
    chatbot_data.loc[(chatbot_data['Emotion'] == "분노"), 'Emotion'] = 2  #분노 => 2
    chatbot_data.loc[(chatbot_data['Emotion'] == "슬픔"), 'Emotion'] = 3  #슬픔 => 3
    chatbot_data.loc[(chatbot_data['Emotion'] == "중립"), 'Emotion'] = 4  #중립 => 4
    chatbot_data.loc[(chatbot_data['Emotion'] == "행복"), 'Emotion'] = 5  #행복 => 5
    chatbot_data.loc[(chatbot_data['Emotion'] == "혐오"), 'Emotion'] = 6  #혐오 => 6

    data_list = []
    for q, label in zip(chatbot_data['Sentence'], chatbot_data['Emotion']):
        data = []
        data.append(q)
        data.append(str(label))

        data_list.append(data)

    print(data)
    print(data_list[:10])

#train & test 데이터로 나누기
from sklearn.model_selection import train_test_split
dataset_train, dataset_test = train_test_split(data_list, test_size = 0.2, shuffle = True, random_state = 32)

!pip install transformers[sentencepiece]
!pip install gluonnlp mxnet
!pip install sentencepiece
!pip install torch

import gluonnlp as nlp
import numpy as np
from transformers import AutoTokenizer # This is where the change is applied
from torch.utils.data import Dataset


# Use AutoTokenizer to load the tokenizer
tokenizer = AutoTokenizer.from_pretrained('skt/kobert-base-v1')
# Get vocabulary from the tokenizer
vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)

class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,
                 pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))

#추가
def label_to_int(self, label):
        """Convert string labels to integers."""
        # Assuming your labels are '행복/슬픔', '기쁨', etc.
        # Create a dictionary mapping labels to integers
        label_map = {'행복/슬픔': 0, '기쁨': 1, '슬픔':2, '분노':3, '공포':4, '중립':5}  # Add more labels as needed
        #Map each label to the same values as in your 'train_binary' function of your training set'

        return label_map.get(label, -1)

max_len = 64
batch_size = 64
warmup_ratio = 0.1
num_epochs = 5
max_grad_norm = 1
log_interval = 200
learning_rate =  5e-5

# Ensure the cell defining BERTDataset (ipython-input-29-ef403ca2e03a) is executed first.
# ... code from ipython-input-29-ef403ca2e03a ...


# in ipython-input-3-45b15612e022
# Corrected code: Include 'vocab' in the arguments
data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False)
data_test = BERTDataset(dataset_test, 0, 1, tok, vocab, max_len, True, False)

data_train = BERTDataset(dataset_train, 0, 1, tokenizer, max_len, True, False)  # Removed 'vocab' from arguments
data_test = BERTDataset(dataset_test, 0, 1, tokenizer, max_len, True, False)  # Removed 'vocab' from arguments

# Ensure that the BERTDataset class is defined before this line.
# This usually involves running the previous cell where you defined the class.

#train & test 데이터로 나누기
from sklearn.model_selection import train_test_split
dataset_train, dataset_test = train_test_split(data_list, test_size = 0.2, shuffle = True, random_state = 32)

# Assuming the following code is in a previous cell that has been executed
#tok
enizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
# Assuming 'vocab' is defined somewhere in your code, if not you will need to define it.
#tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False) # This line was previously commented out. Make sure you have the nlp library imported.
#class BERTDataset(Dataset):
#    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,
#                 pad, pair):
#        transform = nlp.data.BERTSentenceTransform(
#            bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)
#
#        self.sentences = [transform([i[sent_idx]]) for i in dataset]
#        self.labels = [np.int32(i[label_idx]) for i in dataset]
#
#    def __getitem__(self, i):
#        return (self.sentences[i] + (self.labels[i], ))
#
#    def __len__(self):
#        return (len(self.labels))

# Assuming the following is in a previous cell that has been executed
#max_len = 64
#batch_size = 64
#warmup_ratio = 0.1
#num_epochs = 5
#max_grad_norm = 1
#log_interval = 200
#learning_rate =  5e-5

# BERTDataset : 각 데이터가 BERT 모델의 입력으로 들어갈 수 있도록 tokenization, int encoding, padding하는 함수
# Remove this line: tok = tokenizer.tokenize

# Assuming tok, vocab, and tokenizer are correctly initialized
# Make sure 'tok' and 'vocab' are defined before this line
data_train = BERTDataset(dataset_train, 0, 1, tokenizer, max_len, True, False)  # Removed 'vocab' from arguments
data_test = BERTDataset(dataset_test, 0, 1, tokenizer, max_len, True, False)  # Removed 'vocab' from arguments

max_len = 64
batch_size = 64
warmup_ratio = 0.1
num_epochs = 5
max_grad_norm = 1
log_interval = 200
learning_rate =  5e-5
# BERTDataset : 각 데이터가 BERT 모델의 입력으로 들어갈 수 있도록 tokenization, int encoding, padding하는 함수
# Remove this line: tok = tokenizer.tokenize

data_train = BERTDataset(dataset_train, 0, 1, tokenizer, max_len, True, False)  # Removed 'vocab' from arguments
data_test = BERTDataset(dataset_test, 0, 1, tokenizer, max_len, True, False)  # Removed 'vocab' from arguments

!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master

!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'

from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel

from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup

tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)
vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')

from torch.utils.data import Dataset  # Import the Dataset class

class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,
                 pad, pair):
        # Remove the vocab argument from BERTSentenceTransform
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))

# BERT 토크나이저 초기화
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)

# BERTDataset 클래스 정의
class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,
                 pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return len(self.labels)

# 데이터셋 생성
max_len = 64
data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False)
data_test = BERTDataset(dataset_test, 0, 1, tok, vocab, max_len, True, False)

tok = tokenizer.tokenize

data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False)
data_test = BERTDataset(dataset_test, 0, 1, tok, vocab, max_len, True, False)

!pip install torch transformers gluonnlp #추가한 복합 감정 분석 데이터

import pandas as pd

# 복합 감정 데이터 예시 (각 문장에 여러 감정 표시 가능)
data = {
    "Sentence": [
        "오늘 너무 행복하지만 조금 슬퍼요.",
        "정말 화가 나는데 또 놀랍기도 해요.",
        "무서워서 도망가고 싶었어요.",
    ],
    "Labels": [
        [0, 0, 0, 1, 1, 0, 0],  # 슬픔, 행복
        [0, 1, 1, 0, 0, 0, 0],  # 놀람, 분노
        [1, 0, 0, 0, 0, 0, 0],  # 공포
    ]
}

df = pd.DataFrame(data)

import torch
from torch.utils.data import Dataset

class MultiLabelDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.sentences = dataframe["Sentence"].tolist()
        self.labels = dataframe["Labels"].tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        labels = torch.tensor(self.labels[idx], dtype=torch.float)
        encoding = self.tokenizer(
            sentence,
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": labels,
        }

from transformers import BertModel

class MultiLabelBERT(nn.Module):
    def __init__(self, pretrained_model_name, num_labels):
        super(MultiLabelBERT, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs[1]
        return self.classifier(pooled_output)

import torch
from torch.utils.data import DataLoader
from transformers import AdamW
from torch.nn import BCEWithLogitsLoss, Module  # Import Module

# Hyperparameters
MAX_LEN = 64
BATCH_SIZE = 16
EPOCHS = 3
LEARNING_RATE = 5e-5

# Tokenizer & Dataset
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
dataset = MultiLabelDataset(df, tokenizer, MAX_LEN)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# Specify device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Define device here

# Model & Optimizer
# Inherit from Module for MultiLabelBERT
class MultiLabelBERT(Module):
    def __init__(self, pretrained_model_name, num_labels):
        super(MultiLabelBERT, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs[1]  # Use outputs.pooler_output or outputs[1] based on the model output
        return self.classifier(pooled_output)

model = MultiLabelBERT('bert-base-multilingual-cased', num_labels=7).to(device)
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
loss_fn = BCEWithLogitsLoss()

# Training Loop
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}")

def predict(sentence, model, tokenizer, threshold=0.5):
    encoding = tokenizer(sentence, padding="max_length", truncation=True, max_length=MAX_LEN, return_tensors="pt")
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    model.eval()
    with torch.no_grad():
        logits = model(input_ids, attention_mask)
        probabilities = torch.sigmoid(logits).squeeze(0).cpu().numpy()

    labels = [idx for idx, prob in enumerate(probabilities) if prob > threshold]
    return labels, probabilities

# 예측 예시
sentence = "오늘 너무 행복하지만 조금 슬퍼요."
labels, probs = predict(sentence, model, tokenizer)
print(f"Detected labels: {labels}, Probabilities: {probs}")

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel, AdamW
from torch.nn import BCEWithLogitsLoss
import pandas as pd

# 데이터 준비
data = {
    "Sentence": [
        "오늘 너무 행복하지만 조금 슬퍼요.",
        "정말 화가 나는데 또 놀랍기도 해요.",
        "무서워서 도망가고 싶었어요.",
    ],
    "Labels": [
        [0, 0, 0, 1, 1, 0, 0],  # 슬픔, 행복
        [0, 1, 1, 0, 0, 0, 0],  # 놀람, 분노
        [1, 0, 0, 0, 0, 0, 0],  # 공포
    ]
}

df = pd.DataFrame(data)

# 다중 레이블 데이터셋 클래스
class MultiLabelDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.sentences = dataframe["Sentence"].tolist()
        self.labels = dataframe["Labels"].tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        labels = torch.tensor(self.labels[idx], dtype=torch.float)
        encoding = self.tokenizer(
            sentence,
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": labels,
        }

# 다중 감정 분석 BERT 모델 클래스
class MultiLabelBERT(nn.Module):
    def __init__(self, pretrained_model_name, num_labels):
        super(MultiLabelBERT, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        return self.classifier(pooled_output)

# 설정
MAX_LEN = 64
BATCH_SIZE = 16
EPOCHS = 10
LEARNING_RATE = 5e-5
LABELS_MAP = ["공포", "놀람", "분노", "슬픔", "중립", "행복", "혐오"]

# 토크나이저 및 데이터 준비
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
dataset = MultiLabelDataset(df, tokenizer, MAX_LEN)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# 모델 준비
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MultiLabelBERT('bert-base-multilingual-cased', num_labels=len(LABELS_MAP)).to(device)
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
loss_fn = BCEWithLogitsLoss()

# 학습 루프
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}")

# 예측 함수
def predict(sentence, model, tokenizer, threshold=0.5):
    encoding = tokenizer(sentence, padding="max_length", truncation=True, max_length=MAX_LEN, return_tensors="pt")
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    model.eval()
    with torch.no_grad():
        logits = model(input_ids, attention_mask)
        probabilities = torch.sigmoid(logits).squeeze(0).cpu().numpy()

    # 감정 태그 추출
    detected_labels = [LABELS_MAP[idx] for idx, prob in enumerate(probabilities) if prob > threshold]
    return detected_labels, probabilities

# 사용 예제
# 예측 함수 정의는 기존 코드와 동일합니다.

# 사용 예제
try:
    while True:
        sentence = input("감정을 분석할 문장을 입력하세요 ('exit' 입력 시 종료): ")
        if sentence.lower() == "exit":
            print("프로그램을 종료합니다.")
            break
        labels, probs = predict(sentence, model, tokenizer)
        print(f"감지된 감정: {', '.join(labels)}")
except KeyboardInterrupt:
    print("\n프로그램이 사용자의 요청에 의해 종료되었습니다.")

from tqdm import tqdm
for epoch in range(EPOCHS):
    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}")
    for batch in progress_bar:
        ...

from torch.cuda.amp import GradScaler, autocast

scaler = GradScaler()

for epoch in range(EPOCHS):
    model.train()
    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()
        with autocast():
            outputs = model(input_ids, attention_mask)
            loss = loss_fn(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel, AdamW
from torch.nn import BCEWithLogitsLoss
import pandas as pd

# 데이터 로드
file_path = "/content/한국어_단발성_대화_데이터셋.xlsx"
df = pd.read_excel(file_path)

# 데이터 전처리: "Emotion" 열의 클래스 값을 리스트로 변환
# 레이블 매핑 정의
LABELS_MAP = ["공포", "놀람", "분노", "슬픔", "중립", "행복", "혐오"]
num_labels = len(LABELS_MAP)

# Emotion 열을 멀티라벨 벡터로 변환
def convert_labels(emotion_label):
    one_hot = [0] * num_labels
    for label in emotion_label.split(","):  # 감정이 ','로 구분된 경우 처리
        label = label.strip()
        if label in LABELS_MAP:
            one_hot[LABELS_MAP.index(label)] = 1
    return one_hot

df["Labels"] = df["Emotion"].apply(convert_labels)

# 다중 레이블 데이터셋 클래스
class MultiLabelDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.sentences = dataframe["Sentence"].tolist()
        self.labels = dataframe["Labels"].tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        labels = torch.tensor(self.labels[idx], dtype=torch.float)
        encoding = self.tokenizer(
            sentence,
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": labels,
        }

# 다중 감정 분석 BERT 모델 클래스
class MultiLabelBERT(nn.Module):
    def __init__(self, pretrained_model_name, num_labels):
        super(MultiLabelBERT, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        return self.classifier(pooled_output)

# 설정
MAX_LEN = 64
BATCH_SIZE = 4
EPOCHS = 3
LEARNING_RATE = 1e-5

# 토크나이저 및 데이터 준비
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
dataset = MultiLabelDataset(df, tokenizer, MAX_LEN)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

df_sample = df.sample(5000, random_state=42)  # 5000개의 데이터만 사용
dataset = MultiLabelDataset(df_sample, tokenizer, MAX_LEN)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# 모델 준비
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MultiLabelBERT('bert-base-multilingual-cased', num_labels=num_labels).to(device)
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
loss_fn = BCEWithLogitsLoss()

# 학습 루프
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}")

# 예측 함수
def predict(sentence, model, tokenizer, threshold=0.5):
    encoding = tokenizer(sentence, padding="max_length", truncation=True, max_length=MAX_LEN, return_tensors="pt")
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    model.eval()
    with torch.no_grad():
        logits = model(input_ids, attention_mask)
        probabilities = torch.sigmoid(logits).squeeze(0).cpu().numpy()

    # 감정 태그 추출
    detected_labels = [LABELS_MAP[idx] for idx, prob in enumerate(probabilities) if prob > threshold]
    return detected_labels, probabilities

# 사용 예제
try:
    while True:
        sentence = input("감정을 분석할 문장을 입력하세요 ('exit' 입력 시 종료): ")
        if sentence.lower() == "exit":
            print("프로그램을 종료합니다.")
            break
        labels, probs = predict(sentence, model, tokenizer)
        print(f"감지된 감정: {', '.join(labels)}")
except KeyboardInterrupt:
    print("\n프로그램이 사용자의 요청에 의해 종료되었습니다.")

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup, TrainingArguments, Trainer
from torch.nn import BCEWithLogitsLoss
import pandas as pd
from sklearn.model_selection import train_test_split
from tqdm.auto import tqdm
from torch.cuda.amp import autocast, GradScaler

# 데이터 로드
file_path = "/content/한국어_단발성_대화_데이터셋.xlsx"
df = pd.read_excel(file_path)

# 데이터 전처리: "Emotion" 열의 클래스 값을 리스트로 변환
# 레이블 매핑 정의
LABELS_MAP = ["공포", "놀람", "분노", "슬픔", "중립", "행복", "혐오"]
num_labels = len(LABELS_MAP)

# Emotion 열을 멀티라벨 벡터로 변환
def convert_labels(emotion_label):
    one_hot = [0] * num_labels
    for label in emotion_label.split(","):  # 감정이 ','로 구분된 경우 처리
        label = label.strip()
        if label in LABELS_MAP:
            one_hot[LABELS_MAP.index(label)] = 1
    return one_hot

df["Labels"] = df["Emotion"].apply(convert_labels)

# 다중 레이블 데이터셋 클래스
class MultiLabelDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.sentences = dataframe["Sentence"].tolist()
        self.labels = dataframe["Labels"].tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        labels = torch.tensor(self.labels[idx], dtype=torch.float)
        encoding = self.tokenizer(
            sentence,
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": labels,
        }

# 설정
MAX_LEN = 64
BATCH_SIZE = 4
EPOCHS = 1
LEARNING_RATE = 1e-5

# 토크나이저 및 데이터 준비
# Use AutoTokenizer to ensure tokenizer matches the model
tokenizer = AutoTokenizer.from_pretrained('monologg/koelectra-base-discriminator')
dataset = MultiLabelDataset(df, tokenizer, MAX_LEN)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# 데이터셋 샘플링
train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)
train_dataset = MultiLabelDataset(train_df.sample(5000), tokenizer, MAX_LEN)  # 5000개만 사용
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)  # num_workers=4 사용
eval_dataset = MultiLabelDataset(eval_df, tokenizer, MAX_LEN)
eval_loader = DataLoader(eval_dataset, batch_size=4)

# 혼합 정밀도 학습
scaler = GradScaler()

# 모델 준비
model = MultiLabelBERT('monologg/koelectra-base-discriminator', num_labels=num_labels).to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# 학습 루프
for epoch in range(1):  # 에포크 1로 제한
    model.train()
    total_loss = 0
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
    for batch_idx, batch in enumerate(progress_bar):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device).to(torch.float)

        optimizer.zero_grad()
        with autocast():
            outputs = model(input_ids, attention_mask)
            loss = loss_fn(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()
        progress_bar.set_postfix({"Loss": loss.item()})

    print(f"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}")

# 예측 함수
def predict(sentence, model, tokenizer, threshold=0.5):
    encoding = tokenizer(sentence, padding="max_length", truncation=True, max_length=MAX_LEN, return_tensors="pt")
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    model.eval()
    with torch.no_grad():
        logits = model(input_ids, attention_mask)
        probabilities = torch.sigmoid(logits).squeeze(0).cpu().numpy()

    # 감정 태그 추출
    detected_labels = [LABELS_MAP[idx] for idx, prob in enumerate(probabilities) if prob > threshold]
    return detected_labels, probabilities

try:
    while True:
        sentence = input("감정을 분석할 문장을 입력하세요 ('exit' 입력 시 종료): ")
        if sentence.lower() == "exit":
            print("프로그램을 종료합니다.")
            break
        labels, probs = predict(sentence, model, tokenizer)
        print(f"감지된 감정: {', '.join(labels)}")
except KeyboardInterrupt:
    print("\n프로그램이 사용자의 요청에 의해 종료되었습니다.")

import os
if os.path.exists(file_path):
    print("파일이 존재합니다.")
else:
    print("파일이 존재하지 않습니다.")

import os
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW
import pandas as pd
from sklearn.model_selection import train_test_split
from tqdm.auto import tqdm

# 데이터 로드
file_path = "/content/한국어_단발성_대화_데이터셋.xlsx"
df = pd.read_excel(file_path)

# 데이터 전처리: "Emotion" 열의 클래스 값을 리스트로 변환
LABELS_MAP = ["공포", "놀람", "분노", "슬픔", "중립", "행복", "혐오"]
num_labels = len(LABELS_MAP)

def convert_labels(emotion_label):
    one_hot = [0] * num_labels
    for label in emotion_label.split(","):
        label = label.strip()
        if label in LABELS_MAP:
            one_hot[LABELS_MAP.index(label)] = 1
    return one_hot

df["Labels"] = df["Emotion"].apply(convert_labels)

# 다중 레이블 데이터셋 클래스
class MultiLabelDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.sentences = dataframe["Sentence"].tolist()
        self.labels = dataframe["Labels"].tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        labels = torch.tensor(self.labels[idx], dtype=torch.float)
        encoding = self.tokenizer(
            sentence,
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": labels,
        }

# 설정
MODEL_PATH = "koelectra_multilabel_model.pt"  # 모델 저장 경로
MAX_LEN = 64
BATCH_SIZE = 4
EPOCHS = 1
LEARNING_RATE = 1e-5

# 토크나이저
tokenizer = AutoTokenizer.from_pretrained('monologg/koelectra-base-discriminator')

# 모델 정의
class MultiLabelBERT(nn.Module):
    def __init__(self, pretrained_model_name, num_labels):
        super(MultiLabelBERT, self).__init__()
        self.bert = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=num_labels)

    def forward(self, input_ids, attention_mask):
        return self.bert(input_ids=input_ids, attention_mask=attention_mask).logits

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MultiLabelBERT('monologg/koelectra-base-discriminator', num_labels=num_labels).to(device)

# 데이터셋 분리
train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)
train_dataset = MultiLabelDataset(train_df, tokenizer, MAX_LEN)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

# 모델 학습 및 저장
if not os.path.exists(MODEL_PATH):
    print("학습된 모델이 없습니다. 학습을 시작합니다...")
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
    loss_fn = nn.BCEWithLogitsLoss()

    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        for batch in progress_bar:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            optimizer.zero_grad()
            logits = model(input_ids, attention_mask)
            loss = loss_fn(logits, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            progress_bar.set_postfix({"Loss": loss.item()})

        print(f"Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader):.4f}")

    # 모델 저장
    torch.save(model.state_dict(), MODEL_PATH)
    print(f"모델이 저장되었습니다: {MODEL_PATH}")
else:
    print("저장된 모델을 로드합니다...")
    model.load_state_dict(torch.load(MODEL_PATH))
    model.to(device)

# 예측 함수
def predict(sentence, model, tokenizer, threshold=0.5):
    model.eval()
    encoding = tokenizer(sentence, padding="max_length", truncation=True, max_length=MAX_LEN, return_tensors="pt")
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    with torch.no_grad():
        logits = model(input_ids, attention_mask)
        probabilities = torch.sigmoid(logits).squeeze(0).cpu().numpy()

    detected_labels = [LABELS_MAP[idx] for idx, prob in enumerate(probabilities) if prob > threshold]
    return detected_labels, probabilities

# 실시간 예측
try:
    while True:
        sentence = input("감정을 분석할 문장을 입력하세요 ('exit' 입력 시 종료): ")
        if sentence.lower() == "exit":
            print("프로그램을 종료합니다.")
            break
        labels, probs = predict(sentence, model, tokenizer)
        print(f"감지된 감정: {', '.join(labels)}")
except KeyboardInterrupt:
    print("\n프로그램이 사용자의 요청에 의해 종료되었습니다.")